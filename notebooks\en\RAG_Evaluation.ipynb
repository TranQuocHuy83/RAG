{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+o/Yv3b/yKWWMlKjOj7xN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranQuocHuy83/RAG/blob/colab-dev/notebooks%5Cen%5CRAG_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ƒê√°nh gi√° RAG**\n",
        "_T√°c gi·∫£: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "S·ªï tay n√†y minh h·ªça c√°ch b·∫°n c√≥ th·ªÉ ƒë√°nh gi√° h·ªá th·ªëng RAG (Retrieval Augmented Generation-Th·∫ø h·ªá tƒÉng c∆∞·ªùng truy su·∫•t) c·ªßa m√¨nh b·∫±ng c√°ch x√¢y d·ª±ng m·ªôt t·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° t·ªïng h·ª£p v√† s·ª≠ d·ª•ng LLM l√†m c√¥ng c·ª• ƒë√°nh gi√° ƒë·ªÉ t√≠nh to√°n ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng.\n",
        "\n",
        "ƒê·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ RAG, b·∫°n c√≥ th·ªÉ xem [s·ªï tay kh√°c n√†y](rag_zephyr_langchain)!\n",
        "\n",
        "H·ªá th·ªëng RAG r·∫•t ph·ª©c t·∫°p: ƒë√¢y l√† s∆° ƒë·ªì RAG, trong ƒë√≥ ch√∫ng t√¥i ƒë√£ ƒë√°nh d·∫•u m√†u xanh lam t·∫•t c·∫£ c√°c kh·∫£ nƒÉng c·∫£i ti·∫øn h·ªá th·ªëng:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "Vi·ªác tri·ªÉn khai b·∫•t k·ª≥ c·∫£i ti·∫øn n√†o trong s·ªë n√†y ƒë·ªÅu c√≥ th·ªÉ mang l·∫°i hi·ªáu su·∫•t tƒÉng ƒë√°ng k·ªÉ; nh∆∞ng vi·ªác thay ƒë·ªïi b·∫•t c·ª© ƒëi·ªÅu g√¨ c≈©ng v√¥ √≠ch n·∫øu b·∫°n kh√¥ng th·ªÉ theo d√µi t√°c ƒë·ªông c·ªßa nh·ªØng thay ƒë·ªïi ƒë√≥ ƒë·∫øn hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng!\n",
        "\n",
        "V·∫≠y h√£y c√πng xem c√°ch ƒë√°nh gi√° h·ªá th·ªëng RAG c·ªßa ch√∫ng ta.\n",
        "\n",
        "### **ƒê√°nh gi√° hi·ªáu su·∫•t c·ªßa RAG**\n",
        "\n",
        "V√¨ c√≥ r·∫•t nhi·ªÅu th√†nh ph·∫ßn c·∫ßn ƒëi·ªÅu ch·ªânh v√† ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn hi·ªáu su·∫•t, vi·ªác ƒë√°nh gi√° hi·ªáu nƒÉng c·ªßa h·ªá th·ªëng RAG l√† r·∫•t quan tr·ªçng.\n",
        "\n",
        "ƒê·ªÉ x√¢y d·ª±ng quy tr√¨nh ƒë√°nh gi√°, ch√∫ng ta c·∫ßn:\n",
        "1. M·ªôt t·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° v·ªõi c√°c c·∫∑p c√¢u h·ªèi - c√¢u tr·∫£ l·ªùi (c·∫∑p QA)\n",
        "2. M·ªôt b·ªô ƒë√°nh gi√° ƒë·ªÉ t√≠nh to√°n ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng tr√™n t·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° ·ªü tr√™n.\n",
        "\n",
        "‚û°Ô∏è H√≥a ra, ch√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng LLM ƒë·ªÉ h·ªó tr·ª£ trong su·ªët qu√° tr√¨nh!\n",
        "\n",
        "1. T·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° s·∫Ω ƒë∆∞·ª£c t·∫°o ra m·ªôt c√°ch t·ªïng h·ª£p b·ªüi m·ªôt LLM ü§ñ, v√† c√°c c√¢u h·ªèi s·∫Ω ƒë∆∞·ª£c l·ªçc ra b·ªüi c√°c LLM kh√°c ü§ñ\n",
        "2. M·ªôt t√°c nh√¢n [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) ü§ñ sau ƒë√≥ s·∫Ω th·ª±c hi·ªán ƒë√°nh gi√° tr√™n t·∫≠p d·ªØ li·ªáu t·ªïng h·ª£p n√†y.\n",
        "\n",
        "__H√£y c√πng t√¨m hi·ªÉu v√† b·∫Øt ƒë·∫ßu x√¢y d·ª±ng quy tr√¨nh ƒë√°nh gi√° c·ªßa ch√∫ng ta!__ ƒê·∫ßu ti√™n, ch√∫ng ta c√†i ƒë·∫∑t c√°c ph·ª• thu·ªôc m√¥ h√¨nh c·∫ßn thi·∫øt."
      ],
      "metadata": {
        "id": "ClzgD0SL5B-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoV0dGtT40p2"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille\n",
        "!pip install -U langchain-text-splitters langchain-community langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %reload_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "id": "zSc_UGkW5lRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None) #Hi·ªÉn th·ªã tr·ªçn v·∫πn n·ªôi dung vƒÉn b·∫£n"
      ],
      "metadata": {
        "id": "PKIa809R5n__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "hRwvXGs65p4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load your knowledge base**"
      ],
      "metadata": {
        "id": "-xAB332t5s-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ],
      "metadata": {
        "id": "NxlN6Dhs5t5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)"
      ],
      "metadata": {
        "id": "8dAf3rJE5xUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Chuy·ªÉn 5 d√≤ng ƒë·∫ßu ti√™n sang d·∫°ng b·∫£ng\n",
        "df_view = pd.DataFrame(ds.select(range(5)))\n",
        "display(df_view)"
      ],
      "metadata": {
        "id": "4oHJlMiK5yli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. X√¢y d·ª±ng b·ªô d·ªØ li·ªáu t·ªïng h·ª£p ƒë·ªÉ ƒë√°nh gi√°**\n",
        "Tr∆∞·ªõc ti√™n, ch√∫ng ta x√¢y d·ª±ng m·ªôt b·ªô d·ªØ li·ªáu t·ªïng h·ª£p g·ªìm c√°c c√¢u h·ªèi v√† ng·ªØ c·∫£nh li√™n quan. Ph∆∞∆°ng ph√°p l√† l·∫•y c√°c ph·∫ßn t·ª≠ t·ª´ c∆° s·ªü tri th·ª©c c·ªßa ch√∫ng ta, v√† y√™u c·∫ßu m·ªôt LLM t·∫°o ra c√°c c√¢u h·ªèi d·ª±a tr√™n c√°c t√†i li·ªáu n√†y.\n",
        "\n",
        "Sau ƒë√≥, ch√∫ng ta thi·∫øt l·∫≠p c√°c t√°c nh√¢n LLM kh√°c ƒë·ªÉ ho·∫°t ƒë·ªông nh∆∞ c√°c b·ªô l·ªçc ch·∫•t l∆∞·ª£ng cho c√°c c·∫∑p c√¢u h·ªèi-tr·∫£ l·ªùi ƒë∆∞·ª£c t·∫°o ra: m·ªói t√°c nh√¢n s·∫Ω ho·∫°t ƒë·ªông nh∆∞ m·ªôt b·ªô l·ªçc cho m·ªôt l·ªói c·ª• th·ªÉ."
      ],
      "metadata": {
        "id": "I_7a0D4a51w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1. Prepare source documents**"
      ],
      "metadata": {
        "id": "vWsGzPWP56JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "\n",
        "langchain_docs = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    add_start_index=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "docs_processed = [] # danh s√°ch vƒÉn b·∫£n ng·∫Øn --> chu·∫©n b·ªã chuy·ªÉn th√†nh vector\n",
        "for doc in langchain_docs:\n",
        "    docs_processed += text_splitter.split_documents([doc])\n",
        "# ki·ªÉm tra s·ªë l∆∞·ª£ng vƒÉn b·∫£n\n",
        "print(len(docs_processed))"
      ],
      "metadata": {
        "id": "KbvG7z1951W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2. Thi·∫øt l·∫≠p t√°c nh√¢n ƒë·ªÉ t·∫°o c√¢u h·ªèi**\n",
        "\n",
        "Ch√∫ng t√¥i s·ª≠ d·ª•ng [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) ƒë·ªÉ t·∫°o c·∫∑p c√¢u h·ªèi QA v√¨ n√≥ c√≥ hi·ªáu su·∫•t tuy·ªát v·ªùi tr√™n c√°c b·∫£ng x·∫øp h·∫°ng nh∆∞ [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
      ],
      "metadata": {
        "id": "YKfoyonL5-n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Khai b√°o m√¥ h√¨nh\n",
        "# repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "# repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120) # c√¥ng c·ª• g·ª≠i y√™u c·∫ßu l√™n server\n",
        "\n",
        "\n",
        "def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "  # Nh·∫≠n:\n",
        "  # inference_client ‚Üí k·∫øt n·ªëi server\n",
        "  # prompt ‚Üí n·ªôi dung ng∆∞·ªùi d√πng\n",
        "\n",
        "    # S·ª≠ d·ª•ng chat_completion thay v√¨ text_generation\n",
        "    messages1 = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    response = inference_client.chat_completion(\n",
        "        messages=messages1,\n",
        "        max_tokens=200,\n",
        "        temperature=0.1\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Ch·∫°y th·ª≠ nghi·ªám\n",
        "try:\n",
        "    result = call_llm(llm_client, \"Vi·ªát Nam c√≥ bao nhi√™u t·ªânh/th√†nh?\")\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(f\"L·ªói: {e}\")"
      ],
      "metadata": {
        "id": "LSXhj5fx6Bd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a factoid question and an answer given a context.\n",
        "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
        "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Factoid question: (your factoid question)\n",
        "Answer: (your answer to the factoid question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ],
      "metadata": {
        "id": "wYB4-yHE6Epz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Th·ª≠ nghi·ªám v·ªõi m·ªôt ƒëo·∫°n vƒÉn b·∫£n th·∫≠t trong docs_processed\n",
        "import random\n",
        "\n",
        "# L·∫•y ng·∫´u nhi√™n 1 ƒëo·∫°n trong s·ªë 28.308 ƒëo·∫°n ƒë√£ chia\n",
        "random_idx = random.randint(0, len(docs_processed)-1)\n",
        "sample_context = docs_processed[random_idx].page_content\n",
        "\n",
        "# G·ª≠i cho AI\n",
        "print(f\"--- ƒêO·∫†N VƒÇN G·ªêC ---\\n{sample_context}\\n\")\n",
        "result = call_llm(llm_client, QA_generation_prompt.format(context=sample_context))\n",
        "print(f\"--- AI T·ª∞ ƒê·∫∂T C√ÇU H·ªéI ---\\n{result}\")"
      ],
      "metadata": {
        "id": "bbv8kRwj6GQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B√¢y gi·ªù ch√∫ng ta h√£y t·∫°o c√°c c·∫∑p ki·ªÉm th·ª≠ ch·∫•t l∆∞·ª£ng (QA).\n",
        "\n",
        "Trong v√≠ d·ª• n√†y, ch√∫ng ta ch·ªâ t·∫°o 5 c·∫∑p QA v√† s·∫Ω t·∫£i ph·∫ßn c√≤n l·∫°i t·ª´ Hub.\n",
        "\n",
        "Nh∆∞ng ƒë·ªëi v·ªõi c∆° s·ªü tri th·ª©c c·ª• th·ªÉ c·ªßa b·∫°n, gi·∫£ s·ª≠ b·∫°n mu·ªën c√≥ √≠t nh·∫•t kho·∫£ng 100 m·∫´u th·ª≠ nghi·ªám, v√† t√≠nh ƒë·∫øn vi·ªác ch√∫ng ta s·∫Ω l·ªçc b·ªè kho·∫£ng m·ªôt n·ª≠a s·ªë n√†y b·∫±ng c√°c t√°c nh√¢n ph√™ b√¨nh sau n√†y, b·∫°n n√™n t·∫°o nhi·ªÅu h∆°n, kho·∫£ng >200 m·∫´u."
      ],
      "metadata": {
        "id": "uGabB3Ue6IZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "N_GENERATIONS = 5  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    raw_output = call_llm(\n",
        "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Regex n√†y s·∫Ω t√¨m n·ªôi dung n·∫±m sau 'Question:' v√† 'Answer:' b·∫•t k·ªÉ vi·∫øt hoa hay th∆∞·ªùng\n",
        "        question_match = re.search(r\"(?:Question|Factoid question):\\s*(.*)\", raw_output, re.IGNORECASE)\n",
        "        answer_match = re.search(r\"Answer:\\s*(.*)\", raw_output, re.IGNORECASE)\n",
        "\n",
        "        if question_match and answer_match:\n",
        "            question = question_match.group(1).split(\"Answer:\")[0].strip()\n",
        "            answer = answer_match.group(1).strip()\n",
        "\n",
        "            # Ch·ªâ l∆∞u n·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng qu√° d√†i v√† kh√¥ng b·ªã r·ªóng\n",
        "            if 10 < len(answer) < 500:\n",
        "                outputs.append({\n",
        "                    \"context\": sampled_context.page_content,\n",
        "                    \"question\": question,\n",
        "                    \"answer\": answer,\n",
        "                    \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "                })\n",
        "    except Exception as e:\n",
        "        continue"
      ],
      "metadata": {
        "id": "B8UJFvkZ6Kst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "id": "ONanQyQb6M79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuy·ªÉn th√†nh DataFrame ƒë·ªÉ d·ªÖ quan s√°t\n",
        "display(pd.DataFrame(outputs).head(2))"
      ],
      "metadata": {
        "id": "RSVdjYQr6Obs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3. Thi·∫øt l·∫≠p c√°c t√°c nh√¢n ƒë√°nh gi√°**\n",
        "\n",
        "C√°c c√¢u h·ªèi do t√°c nh√¢n tr∆∞·ªõc ƒë√≥ t·∫°o ra c√≥ th·ªÉ c√≥ nhi·ªÅu l·ªói: ch√∫ng ta n√™n ki·ªÉm tra ch·∫•t l∆∞·ª£ng tr∆∞·ªõc khi x√°c nh·∫≠n c√°c c√¢u h·ªèi n√†y.\n",
        "\n",
        "Do ƒë√≥, ch√∫ng ta x√¢y d·ª±ng c√°c t√°c nh√¢n ƒë√°nh gi√° s·∫Ω x·∫øp h·∫°ng t·ª´ng c√¢u h·ªèi d·ª±a tr√™n m·ªôt s·ªë ti√™u ch√≠, ƒë∆∞·ª£c n√™u trong [b√†i b√°o n√†y](https://huggingface.co/papers/2312.10003):\n",
        "- **T√≠nh x√°c th·ª±c:** c√¢u h·ªèi c√≥ th·ªÉ ƒë∆∞·ª£c tr·∫£ l·ªùi t·ª´ ng·ªØ c·∫£nh ƒë√£ cho kh√¥ng?\n",
        "\n",
        "- **T√≠nh ph√π h·ª£p:** c√¢u h·ªèi c√≥ ph√π h·ª£p v·ªõi ng∆∞·ªùi d√πng kh√¥ng? V√≠ d·ª•, `\"Ng√†y ph√°t h√†nh transformers 4.29.1 l√† khi n√†o?\"` kh√¥ng ph√π h·ª£p v·ªõi nh·ªØng ng∆∞·ªùi th·ª±c h√†nh ML.\n",
        "\n",
        "M·ªôt tr∆∞·ªùng h·ª£p l·ªói cu·ªëi c√πng m√† ch√∫ng t√¥i nh·∫≠n th·∫•y l√† khi m·ªôt h√†m ƒë∆∞·ª£c thi·∫øt k·∫ø ri√™ng cho b·ªëi c·∫£nh c·ª• th·ªÉ n∆°i c√¢u h·ªèi ƒë∆∞·ª£c t·∫°o ra, nh∆∞ng b·∫£n th√¢n n√≥ l·∫°i kh√¥ng th·ªÉ gi·∫£i m√£ ƒë∆∞·ª£c, ch·∫≥ng h·∫°n nh∆∞ `\"T√™n c·ªßa h√†m ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h∆∞·ªõng d·∫´n n√†y l√† g√¨?\"`.\n",
        "Ch√∫ng t√¥i c≈©ng x√¢y d·ª±ng m·ªôt t√°c nh√¢n ph√™ b√¨nh cho c√°c ti√™u ch√≠ n√†y:\n",
        "- **ƒê·ªôc l·∫≠p**: c√¢u h·ªèi c√≥ d·ªÖ hi·ªÉu khi kh√¥ng c√≥ b·∫•t k·ª≥ ng·ªØ c·∫£nh n√†o, ƒë·ªëi v·ªõi ng∆∞·ªùi c√≥ ki·∫øn ‚Äã‚Äãth·ª©c chuy√™n m√¥n/truy c·∫≠p Internet kh√¥ng? Ng∆∞·ª£c l·∫°i v·ªõi ƒëi·ªÅu n√†y l√† c√¢u h·ªèi `H√†m n√†o ƒë∆∞·ª£c s·ª≠ d·ª•ng trong b√†i vi·∫øt n√†y?` ƒë·ªëi v·ªõi m·ªôt c√¢u h·ªèi ƒë∆∞·ª£c t·∫°o ra t·ª´ m·ªôt b√†i vi·∫øt blog c·ª• th·ªÉ.\n",
        "\n",
        "Ch√∫ng t√¥i ch·∫•m ƒëi·ªÉm c√°c h√†m m·ªôt c√°ch c√≥ h·ªá th·ªëng b·∫±ng t·∫•t c·∫£ c√°c t√°c nh√¢n n√†y, v√† b·∫•t c·ª© khi n√†o ƒëi·ªÉm s·ªë qu√° th·∫•p ƒë·ªëi v·ªõi b·∫•t k·ª≥ t√°c nh√¢n n√†o, ch√∫ng t√¥i s·∫Ω lo·∫°i b·ªè c√¢u h·ªèi ƒë√≥ kh·ªèi t·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° c·ªßa m√¨nh.\n",
        "\n",
        "üí° ___Khi y√™u c·∫ßu c√°c t√°c nh√¢n ƒë∆∞a ra ƒëi·ªÉm s·ªë, tr∆∞·ªõc ti√™n ch√∫ng t√¥i y√™u c·∫ßu ch√∫ng ƒë∆∞a ra l√Ω do. ƒêi·ªÅu n√†y s·∫Ω gi√∫p ch√∫ng t√¥i x√°c minh ƒëi·ªÉm s·ªë, nh∆∞ng quan tr·ªçng h∆°n, vi·ªác y√™u c·∫ßu ch√∫ng ƒë∆∞a ra l√Ω do tr∆∞·ªõc ti√™n s·∫Ω cung c·∫•p cho m√¥ h√¨nh nhi·ªÅu token h∆°n ƒë·ªÉ suy nghƒ© v√† x√¢y d·ª±ng c√¢u tr·∫£ l·ªùi tr∆∞·ªõc khi t√≥m t·∫Øt n√≥ th√†nh m·ªôt token ƒëi·ªÉm s·ªë duy nh·∫•t.___\n",
        "\n",
        "B√¢y gi·ªù ch√∫ng t√¥i x√¢y d·ª±ng v√† ch·∫°y c√°c t√°c nh√¢n ph√™ b√¨nh n√†y."
      ],
      "metadata": {
        "id": "aTUbCCMF6Qtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ],
      "metadata": {
        "id": "mxA-TepZ6S8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_llm_judge_output(text):\n",
        "    score_match = re.search(r\"Total rating:\\s*([1-5])\", text, re.IGNORECASE)\n",
        "    eval_match = re.search(\n",
        "        r\"Evaluation:\\s*(.*?)(?:\\n\\s*Total rating:|$)\",\n",
        "        text,\n",
        "        re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    if not score_match or not eval_match:\n",
        "        raise ValueError(\"Cannot parse judge output\")\n",
        "\n",
        "    score = int(score_match.group(1))\n",
        "    evaluation = eval_match.group(1).strip()\n",
        "\n",
        "    return score, evaluation\n"
      ],
      "metadata": {
        "id": "3gBKrKrn6VFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(\n",
        "            llm_client,\n",
        "            question_groundedness_critique_prompt.format(\n",
        "                context=output[\"context\"], question=output[\"question\"]\n",
        "            ),\n",
        "        ),\n",
        "        \"relevance\": call_llm(\n",
        "            llm_client,\n",
        "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "        \"standalone\": call_llm(\n",
        "            llm_client,\n",
        "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "    }\n",
        "    try:\n",
        "      for criterion, evaluation in evaluations.items():\n",
        "        score, eval_text = parse_llm_judge_output(evaluation)\n",
        "\n",
        "        output.update({\n",
        "            f\"{criterion}_score\": score,\n",
        "            f\"{criterion}_eval\": eval_text,\n",
        "        })\n",
        "    except Exception:\n",
        "      continue\n"
      ],
      "metadata": {
        "id": "vlxloJjh6W9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gi·ªù ch√∫ng ta h√£y l·ªçc ra nh·ªØng c√¢u h·ªèi kh√¥ng hay d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa h·ªá th·ªëng ƒë√°nh gi√°:"
      ],
      "metadata": {
        "id": "BNL_kogd6Ze9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)# kh√¥ng ng·∫Øt c√¢u h·ªèi/ c√¢u tr·∫£ l·ªùi\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs) # t·∫°o DataFrame t·ª´ outputs\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 4)\n",
        "    & (generated_questions[\"relevance_score\"] >= 4)\n",
        "    & (generated_questions[\"standalone_score\"] >= 4)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_dataset = datasets.Dataset.from_pandas(\n",
        "    generated_questions, split=\"train\", preserve_index=False\n",
        ")"
      ],
      "metadata": {
        "id": "mXSqWAbw6a6d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}